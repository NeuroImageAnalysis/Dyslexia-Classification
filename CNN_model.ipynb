{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "590d15f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import manipulating files libraries\n",
    "import os, glob\n",
    "\n",
    "# Import graph/image plotting libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style=\"darkgrid\")\n",
    "\n",
    "# Import loading and manipulating neuroimaging data library\n",
    "import nibabel as nib\n",
    "\n",
    "# Import array manipulating libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Import deep learning model libraries\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.models import Model, Sequential\n",
    "\n",
    "from keras.layers import Input, Dense, Flatten, Dropout, Conv2D, MaxPooling2D, BatchNormalization\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "\n",
    "# Import date/time library to save models with date/time\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fca7b6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data path\n",
    "data_path = 'final data/*.nii.gz'\n",
    "# Find all files within the location that matches our search string\n",
    "files = glob.glob(data_path)\n",
    "\n",
    "# Get the labels for our data from our csv file\n",
    "Labels = pd.read_csv('labels.csv')\n",
    "# Define our target from the column \"Labels\" as our y in our model\n",
    "target = Labels['group']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9081763a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120\n",
      "(120, 64, 64, 34)\n"
     ]
    }
   ],
   "source": [
    "# Loop through files and load all data files\n",
    "data_all, images = [], []\n",
    "for data_file in sorted(files):\n",
    "    data = nib.load(data_file).get_fdata()\n",
    "    first_vol = data[:,:,:,0]\n",
    "    first = first_vol / 255\n",
    "    data_all.append(first)\n",
    "    \n",
    "# Convert our list into a numpy array\n",
    "images = np.asarray(data_all)\n",
    "\n",
    "print(len(data_all))\n",
    "print (np.shape(images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c1463e74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.         0.         0.         ... 0.         0.         0.        ]\n",
      "  [0.         0.         0.         ... 0.         0.         0.        ]\n",
      "  [0.         0.         0.         ... 0.         0.         0.        ]\n",
      "  ...\n",
      "  [0.         0.         0.         ... 0.         0.         0.        ]\n",
      "  [0.         0.         0.         ... 0.         0.         0.        ]\n",
      "  [0.         0.         0.         ... 0.         0.         0.        ]]\n",
      "\n",
      " [[0.03921569 0.04705882 0.05098039 ... 0.03529412 0.04313725 0.03137255]\n",
      "  [0.03921569 0.03529412 0.05098039 ... 0.04313725 0.03529412 0.04313725]\n",
      "  [0.04705882 0.05098039 0.04313725 ... 0.05098039 0.03529412 0.04313725]\n",
      "  ...\n",
      "  [0.0745098  0.09411765 0.10196078 ... 0.05882353 0.05098039 0.05882353]\n",
      "  [0.08235294 0.06666667 0.09411765 ... 0.05098039 0.05098039 0.05490196]\n",
      "  [0.07058824 0.0745098  0.09019608 ... 0.05490196 0.05490196 0.05882353]]\n",
      "\n",
      " [[0.03921569 0.05098039 0.06666667 ... 0.05098039 0.03921569 0.03921569]\n",
      "  [0.04705882 0.05490196 0.05882353 ... 0.05098039 0.04705882 0.04313725]\n",
      "  [0.04705882 0.05098039 0.05098039 ... 0.04313725 0.04313725 0.04313725]\n",
      "  ...\n",
      "  [0.08627451 0.07058824 0.08627451 ... 0.05490196 0.06666667 0.05098039]\n",
      "  [0.07843137 0.09411765 0.07843137 ... 0.05882353 0.05098039 0.05882353]\n",
      "  [0.06666667 0.07058824 0.09019608 ... 0.05882353 0.05882353 0.05490196]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0.05098039 0.05490196 0.05490196 ... 0.02745098 0.03529412 0.03921569]\n",
      "  [0.04705882 0.05490196 0.04705882 ... 0.03137255 0.03137255 0.03137255]\n",
      "  [0.05490196 0.05882353 0.05490196 ... 0.03137255 0.03137255 0.01960784]\n",
      "  ...\n",
      "  [0.09411765 0.07843137 0.09411765 ... 0.05490196 0.0627451  0.05882353]\n",
      "  [0.08627451 0.0627451  0.09411765 ... 0.0627451  0.05882353 0.05490196]\n",
      "  [0.07058824 0.0745098  0.08235294 ... 0.0627451  0.05098039 0.0627451 ]]\n",
      "\n",
      " [[0.04705882 0.03921569 0.04313725 ... 0.03137255 0.03137255 0.03137255]\n",
      "  [0.05098039 0.05098039 0.04705882 ... 0.02352941 0.03137255 0.02745098]\n",
      "  [0.05098039 0.05098039 0.05882353 ... 0.03529412 0.03529412 0.02745098]\n",
      "  ...\n",
      "  [0.07843137 0.07843137 0.08627451 ... 0.04705882 0.05882353 0.05882353]\n",
      "  [0.07843137 0.09803922 0.08235294 ... 0.0627451  0.0627451  0.05490196]\n",
      "  [0.08627451 0.07058824 0.09803922 ... 0.0627451  0.05490196 0.0627451 ]]\n",
      "\n",
      " [[0.04705882 0.04705882 0.04313725 ... 0.03529412 0.02745098 0.03137255]\n",
      "  [0.05098039 0.04705882 0.05490196 ... 0.03529412 0.03137255 0.02352941]\n",
      "  [0.05882353 0.05098039 0.05490196 ... 0.03529412 0.02745098 0.03137255]\n",
      "  ...\n",
      "  [0.08235294 0.07058824 0.09803922 ... 0.06666667 0.07058824 0.05490196]\n",
      "  [0.07843137 0.08627451 0.0745098  ... 0.05882353 0.05882353 0.05490196]\n",
      "  [0.08235294 0.07843137 0.07058824 ... 0.05882353 0.05098039 0.05882353]]]\n"
     ]
    }
   ],
   "source": [
    "print(images[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f4ab1626",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(84, 64, 64, 34)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create list of indices\n",
    "N = images.shape[0]\n",
    "indexes = np.arange(N)\n",
    "\n",
    "# Divide our dataset into dyslexics and controls to have a balanced train, validation and test sets\n",
    "dis = indexes[:60]\n",
    "con = indexes[60:]\n",
    "\n",
    "#  Cut the dataset at 80% to create the training, 10% validation and 10% test set\n",
    "size = dis.shape[0]\n",
    "split_1 = int(0.7 * size)\n",
    "split_2 = int(0.85 * size)\n",
    "\n",
    "# Shuffle our dyslexics and controls arrays to create random indexes\n",
    "np.random.shuffle(np.asarray(dis))\n",
    "np.random.shuffle(np.asarray(con))\n",
    "\n",
    "# Create our indexes for our train, validation and test sets according to our previous division (80%, 10%, 10%)\n",
    "indexes_train_dis, indexes_train_con = dis[:split_1], con[:split_1]\n",
    "indexes_val_dis, indexes_val_con   = dis[split_1:split_2], con[split_1:split_2]\n",
    "indexes_test_dis, indexes_test_con = dis[split_2:], con[split_2:]\n",
    "\n",
    "# We concatenate our training, validation and test indexes for dyslexics and controls\n",
    "# By doing that we ensure that each set is balanced with the same number of dyslexics and controls\n",
    "indexes_train = np.concatenate((indexes_train_dis, indexes_train_con), axis=None)\n",
    "indexes_val = np.concatenate((indexes_val_dis, indexes_val_con), axis=None)\n",
    "indexes_test = np.concatenate((indexes_test_dis, indexes_test_con), axis=None)\n",
    "\n",
    "\n",
    "# Split the data into training, validation and test sets according to the indexes created previously\n",
    "X_train = images[indexes_train, ...]\n",
    "X_val = images[indexes_val, ...]\n",
    "X_test = images[indexes_test, ...]\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6ef1b05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create outcome variable for each set (training, validation and test)\n",
    "y_train = target[indexes_train] \n",
    "y_val   = target[indexes_val]\n",
    "y_test  = target[indexes_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "847f94be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our vectorized labels\n",
    "y_train = np.asarray(y_train).astype('int32')\n",
    "y_test = np.asarray(y_test).astype('int32')\n",
    "y_val = np.asarray(y_val).astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "46d0c475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]], shape=(18, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "y_train = tf.one_hot(y_train, 2)\n",
    "y_test = tf.one_hot(y_test, 2)\n",
    "y_val = tf.one_hot(y_val, 2)\n",
    "print(y_val) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4f112404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 64, 34)\n"
     ]
    }
   ],
   "source": [
    "# Here we begin to create our sequential model\n",
    "\n",
    "# Get shape of input data\n",
    "data_shape = tuple(X_train.shape[1:])\n",
    "# We check the shape of our data (Nifti files)\n",
    "print(data_shape)\n",
    "\n",
    "# Specify shape of convolution kernel\n",
    "kernel_size = (3, 3)\n",
    "\n",
    "# Specify number of output categories\n",
    "n_classes = 2\n",
    "\n",
    "# Specify number of filters per layer\n",
    "filters = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cba75073",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([84, 2])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "47d12858",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 62, 62, 16)        4912      \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 62, 62, 16)       64        \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 31, 31, 16)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 29, 29, 32)        4640      \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 29, 29, 32)       128       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 14, 14, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 12, 12, 64)        18496     \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 12, 12, 64)       256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 6, 6, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 2304)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                147520    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 64)                0         \n",
      "                                                                 \n",
      " preds (Dense)               (None, 2)                 130       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 176,146\n",
      "Trainable params: 175,922\n",
      "Non-trainable params: 224\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\achin\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\backend.py:451: UserWarning: `tf.keras.backend.set_learning_phase` is deprecated and will be removed after 2020-10-11. To update it, simply pass a True/False value to the `training` argument of the `__call__` method of your layer or model.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "7/7 [==============================] - 2s 120ms/step - loss: 1.3362 - accuracy: 0.4524 - val_loss: 0.7341 - val_accuracy: 0.5000\n",
      "Epoch 2/200\n",
      "7/7 [==============================] - 0s 63ms/step - loss: 0.6225 - accuracy: 0.7381 - val_loss: 0.6953 - val_accuracy: 0.5000\n",
      "Epoch 3/200\n",
      "7/7 [==============================] - 0s 65ms/step - loss: 0.5940 - accuracy: 0.6429 - val_loss: 0.6867 - val_accuracy: 0.5000\n",
      "Epoch 4/200\n",
      "7/7 [==============================] - 0s 64ms/step - loss: 0.4939 - accuracy: 0.7976 - val_loss: 0.6750 - val_accuracy: 0.5000\n",
      "Epoch 5/200\n",
      "7/7 [==============================] - 0s 54ms/step - loss: 0.4286 - accuracy: 0.8452 - val_loss: 0.6585 - val_accuracy: 0.8333\n",
      "Epoch 6/200\n",
      "7/7 [==============================] - 0s 58ms/step - loss: 0.3864 - accuracy: 0.8452 - val_loss: 0.6607 - val_accuracy: 0.7778\n",
      "Epoch 7/200\n",
      "7/7 [==============================] - 0s 63ms/step - loss: 0.2651 - accuracy: 0.9167 - val_loss: 0.6724 - val_accuracy: 0.5000\n",
      "Epoch 8/200\n",
      "7/7 [==============================] - 0s 58ms/step - loss: 0.2757 - accuracy: 0.9048 - val_loss: 0.7000 - val_accuracy: 0.5000\n",
      "Epoch 9/200\n",
      "7/7 [==============================] - 0s 56ms/step - loss: 0.1805 - accuracy: 0.9405 - val_loss: 0.7084 - val_accuracy: 0.5000\n",
      "Epoch 10/200\n",
      "7/7 [==============================] - 0s 56ms/step - loss: 0.1863 - accuracy: 0.9524 - val_loss: 0.6441 - val_accuracy: 0.5000\n",
      "Epoch 11/200\n",
      "7/7 [==============================] - 1s 83ms/step - loss: 0.1795 - accuracy: 0.9286 - val_loss: 0.5753 - val_accuracy: 0.8889\n",
      "Epoch 12/200\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1144 - accuracy: 0.9643 - val_loss: 0.5671 - val_accuracy: 0.7778\n",
      "Epoch 13/200\n",
      "7/7 [==============================] - 1s 80ms/step - loss: 0.1023 - accuracy: 0.9881 - val_loss: 0.5806 - val_accuracy: 0.6111\n",
      "Epoch 14/200\n",
      "7/7 [==============================] - 0s 52ms/step - loss: 0.0888 - accuracy: 0.9762 - val_loss: 0.5895 - val_accuracy: 0.6111\n",
      "Epoch 15/200\n",
      "7/7 [==============================] - 0s 61ms/step - loss: 0.0856 - accuracy: 0.9762 - val_loss: 0.5266 - val_accuracy: 0.8333\n",
      "Epoch 16/200\n",
      "7/7 [==============================] - 0s 56ms/step - loss: 0.0890 - accuracy: 0.9881 - val_loss: 0.6066 - val_accuracy: 0.5000\n",
      "Epoch 17/200\n",
      "7/7 [==============================] - 0s 57ms/step - loss: 0.0411 - accuracy: 0.9881 - val_loss: 0.4647 - val_accuracy: 0.8333\n",
      "Epoch 18/200\n",
      "7/7 [==============================] - 1s 78ms/step - loss: 0.0431 - accuracy: 0.9881 - val_loss: 0.4541 - val_accuracy: 0.8333\n",
      "Epoch 19/200\n",
      "7/7 [==============================] - 0s 52ms/step - loss: 0.0580 - accuracy: 0.9881 - val_loss: 0.5373 - val_accuracy: 0.7222\n",
      "Epoch 20/200\n",
      "7/7 [==============================] - 0s 52ms/step - loss: 0.0355 - accuracy: 0.9881 - val_loss: 0.4553 - val_accuracy: 0.8889\n",
      "Epoch 21/200\n",
      "7/7 [==============================] - 0s 51ms/step - loss: 0.0344 - accuracy: 0.9881 - val_loss: 0.4194 - val_accuracy: 0.8889\n",
      "Epoch 22/200\n",
      "7/7 [==============================] - 0s 52ms/step - loss: 0.0455 - accuracy: 0.9881 - val_loss: 0.4054 - val_accuracy: 0.8889\n",
      "Epoch 23/200\n",
      "7/7 [==============================] - 0s 53ms/step - loss: 0.0335 - accuracy: 0.9881 - val_loss: 0.4036 - val_accuracy: 0.8889\n",
      "Epoch 24/200\n",
      "7/7 [==============================] - 0s 55ms/step - loss: 0.0273 - accuracy: 1.0000 - val_loss: 0.3863 - val_accuracy: 0.8889\n",
      "Epoch 25/200\n",
      "7/7 [==============================] - 0s 55ms/step - loss: 0.0369 - accuracy: 0.9881 - val_loss: 0.3734 - val_accuracy: 0.8889\n",
      "Epoch 26/200\n",
      "7/7 [==============================] - 0s 53ms/step - loss: 0.0398 - accuracy: 0.9881 - val_loss: 0.4590 - val_accuracy: 0.7778\n",
      "Epoch 27/200\n",
      "7/7 [==============================] - 0s 56ms/step - loss: 0.0491 - accuracy: 0.9643 - val_loss: 0.4067 - val_accuracy: 0.8889\n",
      "Epoch 28/200\n",
      "7/7 [==============================] - 0s 54ms/step - loss: 0.0393 - accuracy: 0.9762 - val_loss: 0.5574 - val_accuracy: 0.6667\n",
      "Epoch 29/200\n",
      "7/7 [==============================] - 0s 53ms/step - loss: 0.0320 - accuracy: 0.9762 - val_loss: 0.3880 - val_accuracy: 0.8333\n",
      "Epoch 30/200\n",
      "7/7 [==============================] - 0s 48ms/step - loss: 0.0182 - accuracy: 1.0000 - val_loss: 0.3534 - val_accuracy: 0.8333\n",
      "Epoch 31/200\n",
      "7/7 [==============================] - 0s 48ms/step - loss: 0.0116 - accuracy: 1.0000 - val_loss: 0.3432 - val_accuracy: 0.8333\n",
      "Epoch 32/200\n",
      "7/7 [==============================] - 0s 47ms/step - loss: 0.0131 - accuracy: 1.0000 - val_loss: 0.2743 - val_accuracy: 0.9444\n",
      "Epoch 33/200\n",
      "7/7 [==============================] - 0s 58ms/step - loss: 0.0081 - accuracy: 1.0000 - val_loss: 0.2716 - val_accuracy: 0.8889\n",
      "Epoch 34/200\n",
      "7/7 [==============================] - 0s 49ms/step - loss: 0.0220 - accuracy: 0.9881 - val_loss: 0.3116 - val_accuracy: 0.8333\n",
      "Epoch 35/200\n",
      "7/7 [==============================] - 0s 51ms/step - loss: 0.0186 - accuracy: 0.9881 - val_loss: 0.3554 - val_accuracy: 0.8333\n",
      "Epoch 36/200\n",
      "7/7 [==============================] - 0s 62ms/step - loss: 0.0291 - accuracy: 0.9762 - val_loss: 0.2359 - val_accuracy: 0.8889\n",
      "Epoch 37/200\n",
      "7/7 [==============================] - 0s 54ms/step - loss: 0.0082 - accuracy: 1.0000 - val_loss: 0.5690 - val_accuracy: 0.6667\n",
      "Epoch 38/200\n",
      "7/7 [==============================] - 0s 61ms/step - loss: 0.0123 - accuracy: 1.0000 - val_loss: 0.4965 - val_accuracy: 0.7778\n",
      "Epoch 39/200\n",
      "7/7 [==============================] - 0s 53ms/step - loss: 0.0268 - accuracy: 1.0000 - val_loss: 0.2870 - val_accuracy: 0.8889\n",
      "Epoch 40/200\n",
      "7/7 [==============================] - 0s 54ms/step - loss: 0.0466 - accuracy: 0.9762 - val_loss: 0.4595 - val_accuracy: 0.8333\n",
      "Epoch 41/200\n",
      "7/7 [==============================] - 0s 53ms/step - loss: 0.0204 - accuracy: 0.9881 - val_loss: 0.3283 - val_accuracy: 0.8333\n",
      "Epoch 42/200\n",
      "7/7 [==============================] - 0s 62ms/step - loss: 0.0451 - accuracy: 0.9881 - val_loss: 0.4021 - val_accuracy: 0.7778\n",
      "Epoch 43/200\n",
      "7/7 [==============================] - 0s 61ms/step - loss: 0.0176 - accuracy: 1.0000 - val_loss: 0.3816 - val_accuracy: 0.7778\n",
      "Epoch 44/200\n",
      "7/7 [==============================] - 0s 52ms/step - loss: 0.0195 - accuracy: 0.9881 - val_loss: 0.2129 - val_accuracy: 0.8889\n",
      "Epoch 45/200\n",
      "7/7 [==============================] - 0s 48ms/step - loss: 0.0299 - accuracy: 0.9881 - val_loss: 0.2255 - val_accuracy: 0.9444\n",
      "Epoch 46/200\n",
      "7/7 [==============================] - 0s 47ms/step - loss: 0.0124 - accuracy: 0.9881 - val_loss: 0.2003 - val_accuracy: 0.9444\n",
      "Epoch 47/200\n",
      "7/7 [==============================] - 0s 59ms/step - loss: 0.0077 - accuracy: 1.0000 - val_loss: 0.2453 - val_accuracy: 0.7778\n",
      "Epoch 48/200\n",
      "7/7 [==============================] - 0s 59ms/step - loss: 0.0102 - accuracy: 1.0000 - val_loss: 0.2440 - val_accuracy: 0.7778\n",
      "Epoch 49/200\n",
      "7/7 [==============================] - 0s 55ms/step - loss: 0.0131 - accuracy: 1.0000 - val_loss: 0.1878 - val_accuracy: 0.9444\n",
      "Epoch 50/200\n",
      "7/7 [==============================] - 0s 51ms/step - loss: 0.0282 - accuracy: 0.9881 - val_loss: 0.2491 - val_accuracy: 0.8889\n",
      "Epoch 51/200\n",
      "7/7 [==============================] - 0s 60ms/step - loss: 0.0033 - accuracy: 1.0000 - val_loss: 0.2400 - val_accuracy: 0.9444\n",
      "Epoch 52/200\n",
      "7/7 [==============================] - 0s 53ms/step - loss: 0.0056 - accuracy: 1.0000 - val_loss: 0.2331 - val_accuracy: 0.9444\n",
      "Epoch 53/200\n",
      "7/7 [==============================] - 0s 50ms/step - loss: 0.0109 - accuracy: 1.0000 - val_loss: 0.2699 - val_accuracy: 0.9444\n",
      "Epoch 54/200\n",
      "7/7 [==============================] - 0s 46ms/step - loss: 0.0031 - accuracy: 1.0000 - val_loss: 0.7112 - val_accuracy: 0.6111\n",
      "Epoch 55/200\n",
      "7/7 [==============================] - 0s 48ms/step - loss: 0.0133 - accuracy: 0.9881 - val_loss: 0.7001 - val_accuracy: 0.6111\n",
      "Epoch 56/200\n",
      "7/7 [==============================] - 0s 50ms/step - loss: 0.0254 - accuracy: 0.9762 - val_loss: 0.2860 - val_accuracy: 0.7778\n",
      "Epoch 57/200\n",
      "7/7 [==============================] - 0s 49ms/step - loss: 0.0612 - accuracy: 0.9762 - val_loss: 0.4813 - val_accuracy: 0.7778\n",
      "Epoch 58/200\n",
      "7/7 [==============================] - 0s 50ms/step - loss: 0.0087 - accuracy: 1.0000 - val_loss: 0.3638 - val_accuracy: 0.8333\n",
      "Epoch 59/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 47ms/step - loss: 0.0049 - accuracy: 1.0000 - val_loss: 0.3482 - val_accuracy: 0.7778\n",
      "Epoch 60/200\n",
      "7/7 [==============================] - 0s 49ms/step - loss: 0.0170 - accuracy: 1.0000 - val_loss: 0.3250 - val_accuracy: 0.8333\n",
      "Epoch 61/200\n",
      "7/7 [==============================] - 0s 52ms/step - loss: 0.0176 - accuracy: 0.9881 - val_loss: 0.2985 - val_accuracy: 0.8889\n",
      "Epoch 62/200\n",
      "7/7 [==============================] - 0s 53ms/step - loss: 0.0038 - accuracy: 1.0000 - val_loss: 0.3158 - val_accuracy: 0.8889\n",
      "Epoch 63/200\n",
      "7/7 [==============================] - 0s 52ms/step - loss: 0.0096 - accuracy: 0.9881 - val_loss: 0.4547 - val_accuracy: 0.8333\n",
      "Epoch 64/200\n",
      "7/7 [==============================] - 0s 50ms/step - loss: 0.0150 - accuracy: 1.0000 - val_loss: 0.4447 - val_accuracy: 0.8333\n",
      "Epoch 65/200\n",
      "7/7 [==============================] - 0s 52ms/step - loss: 0.0427 - accuracy: 0.9643 - val_loss: 1.7809 - val_accuracy: 0.6111\n",
      "Epoch 66/200\n",
      "7/7 [==============================] - 0s 54ms/step - loss: 0.0143 - accuracy: 1.0000 - val_loss: 0.6768 - val_accuracy: 0.7778\n",
      "Epoch 67/200\n",
      "7/7 [==============================] - 0s 59ms/step - loss: 0.0227 - accuracy: 0.9881 - val_loss: 0.4077 - val_accuracy: 0.7778\n",
      "Epoch 68/200\n",
      "7/7 [==============================] - 0s 60ms/step - loss: 0.0329 - accuracy: 0.9881 - val_loss: 0.3696 - val_accuracy: 0.8333\n",
      "Epoch 69/200\n",
      "7/7 [==============================] - 0s 54ms/step - loss: 0.1921 - accuracy: 0.9524 - val_loss: 0.8581 - val_accuracy: 0.7778\n",
      "Epoch 70/200\n",
      "7/7 [==============================] - 0s 51ms/step - loss: 0.1558 - accuracy: 0.9405 - val_loss: 0.7511 - val_accuracy: 0.8333\n",
      "Epoch 71/200\n",
      "7/7 [==============================] - 0s 49ms/step - loss: 0.2264 - accuracy: 0.9405 - val_loss: 3.1240 - val_accuracy: 0.6111\n",
      "Epoch 72/200\n",
      "7/7 [==============================] - 0s 49ms/step - loss: 0.0825 - accuracy: 0.9762 - val_loss: 2.0825 - val_accuracy: 0.6111\n",
      "Epoch 73/200\n",
      "7/7 [==============================] - 0s 52ms/step - loss: 0.0657 - accuracy: 0.9762 - val_loss: 2.3385 - val_accuracy: 0.6111\n",
      "Epoch 74/200\n",
      "7/7 [==============================] - 0s 55ms/step - loss: 0.1226 - accuracy: 0.9643 - val_loss: 2.8931 - val_accuracy: 0.5556\n",
      "Epoch 75/200\n",
      "7/7 [==============================] - 0s 58ms/step - loss: 0.1893 - accuracy: 0.9048 - val_loss: 2.4798 - val_accuracy: 0.5556\n",
      "Epoch 76/200\n",
      "7/7 [==============================] - 0s 47ms/step - loss: 0.1472 - accuracy: 0.9286 - val_loss: 0.9134 - val_accuracy: 0.6667\n",
      "Epoch 77/200\n",
      "7/7 [==============================] - 0s 55ms/step - loss: 0.1559 - accuracy: 0.9048 - val_loss: 1.0758 - val_accuracy: 0.6667\n",
      "Epoch 78/200\n",
      "7/7 [==============================] - 0s 55ms/step - loss: 0.0517 - accuracy: 0.9881 - val_loss: 0.5678 - val_accuracy: 0.7778\n",
      "Epoch 79/200\n",
      "7/7 [==============================] - 0s 51ms/step - loss: 0.0771 - accuracy: 0.9643 - val_loss: 0.7419 - val_accuracy: 0.7222\n",
      "Epoch 80/200\n",
      "7/7 [==============================] - 0s 59ms/step - loss: 0.0267 - accuracy: 0.9881 - val_loss: 0.8798 - val_accuracy: 0.6667\n",
      "Epoch 81/200\n",
      "7/7 [==============================] - 0s 50ms/step - loss: 0.0116 - accuracy: 1.0000 - val_loss: 0.8344 - val_accuracy: 0.7222\n",
      "Epoch 82/200\n",
      "7/7 [==============================] - 0s 53ms/step - loss: 0.0640 - accuracy: 0.9881 - val_loss: 2.0305 - val_accuracy: 0.6111\n",
      "Epoch 83/200\n",
      "7/7 [==============================] - 0s 52ms/step - loss: 0.0614 - accuracy: 0.9762 - val_loss: 1.5451 - val_accuracy: 0.7222\n",
      "Epoch 84/200\n",
      "7/7 [==============================] - 0s 71ms/step - loss: 0.0327 - accuracy: 0.9762 - val_loss: 1.3409 - val_accuracy: 0.6667\n",
      "Epoch 85/200\n",
      "7/7 [==============================] - 0s 56ms/step - loss: 0.0129 - accuracy: 1.0000 - val_loss: 1.3605 - val_accuracy: 0.6667\n",
      "Epoch 86/200\n",
      "7/7 [==============================] - 1s 86ms/step - loss: 0.0086 - accuracy: 1.0000 - val_loss: 1.4306 - val_accuracy: 0.6667\n",
      "Epoch 87/200\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 0.0187 - accuracy: 1.0000 - val_loss: 1.4391 - val_accuracy: 0.6667\n",
      "Epoch 88/200\n",
      "7/7 [==============================] - 1s 79ms/step - loss: 0.0069 - accuracy: 1.0000 - val_loss: 1.2638 - val_accuracy: 0.6667\n",
      "Epoch 89/200\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 1.1566 - val_accuracy: 0.6667\n",
      "Epoch 90/200\n",
      "7/7 [==============================] - 0s 73ms/step - loss: 0.0216 - accuracy: 1.0000 - val_loss: 0.9707 - val_accuracy: 0.7222\n",
      "Epoch 91/200\n",
      "7/7 [==============================] - 0s 68ms/step - loss: 0.0145 - accuracy: 0.9881 - val_loss: 0.9432 - val_accuracy: 0.7778\n",
      "Epoch 92/200\n",
      "7/7 [==============================] - 0s 71ms/step - loss: 0.0055 - accuracy: 1.0000 - val_loss: 1.0331 - val_accuracy: 0.7778\n",
      "Epoch 93/200\n",
      "7/7 [==============================] - 0s 68ms/step - loss: 0.0073 - accuracy: 1.0000 - val_loss: 1.2041 - val_accuracy: 0.7778\n",
      "Epoch 94/200\n",
      "7/7 [==============================] - 0s 67ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 1.3124 - val_accuracy: 0.7778\n",
      "Epoch 95/200\n",
      "7/7 [==============================] - 0s 66ms/step - loss: 0.0068 - accuracy: 1.0000 - val_loss: 1.3412 - val_accuracy: 0.7778\n",
      "Epoch 96/200\n",
      "7/7 [==============================] - 1s 74ms/step - loss: 0.0183 - accuracy: 1.0000 - val_loss: 1.2847 - val_accuracy: 0.8333\n",
      "Epoch 97/200\n",
      "7/7 [==============================] - 0s 66ms/step - loss: 9.3372e-04 - accuracy: 1.0000 - val_loss: 1.1785 - val_accuracy: 0.8333\n",
      "Epoch 98/200\n",
      "7/7 [==============================] - 0s 71ms/step - loss: 0.0129 - accuracy: 1.0000 - val_loss: 1.3021 - val_accuracy: 0.8333\n",
      "Epoch 99/200\n",
      "7/7 [==============================] - 1s 72ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 1.4833 - val_accuracy: 0.7778\n",
      "Epoch 100/200\n",
      "7/7 [==============================] - 1s 76ms/step - loss: 0.0090 - accuracy: 1.0000 - val_loss: 1.5781 - val_accuracy: 0.7778\n",
      "Epoch 101/200\n",
      "7/7 [==============================] - 0s 70ms/step - loss: 0.0321 - accuracy: 0.9762 - val_loss: 1.6848 - val_accuracy: 0.7778\n",
      "Epoch 102/200\n",
      "7/7 [==============================] - 0s 59ms/step - loss: 0.0170 - accuracy: 0.9881 - val_loss: 1.4652 - val_accuracy: 0.7222\n",
      "Epoch 103/200\n",
      "7/7 [==============================] - 0s 64ms/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 1.3456 - val_accuracy: 0.7222\n",
      "Epoch 104/200\n",
      "7/7 [==============================] - 0s 70ms/step - loss: 0.0086 - accuracy: 1.0000 - val_loss: 1.2773 - val_accuracy: 0.8333\n",
      "Epoch 105/200\n",
      "7/7 [==============================] - 0s 69ms/step - loss: 0.0569 - accuracy: 0.9881 - val_loss: 1.8879 - val_accuracy: 0.7222\n",
      "Epoch 106/200\n",
      "7/7 [==============================] - 1s 73ms/step - loss: 0.0245 - accuracy: 0.9881 - val_loss: 1.5921 - val_accuracy: 0.6667\n",
      "Epoch 107/200\n",
      "7/7 [==============================] - 0s 73ms/step - loss: 0.0580 - accuracy: 0.9881 - val_loss: 0.9208 - val_accuracy: 0.8333\n",
      "Epoch 108/200\n",
      "7/7 [==============================] - 1s 74ms/step - loss: 0.0536 - accuracy: 0.9762 - val_loss: 1.1371 - val_accuracy: 0.8333\n",
      "Epoch 109/200\n",
      "7/7 [==============================] - 0s 68ms/step - loss: 0.0113 - accuracy: 1.0000 - val_loss: 1.0757 - val_accuracy: 0.8333\n",
      "Epoch 110/200\n",
      "7/7 [==============================] - 1s 75ms/step - loss: 0.0244 - accuracy: 0.9881 - val_loss: 0.9444 - val_accuracy: 0.8333\n",
      "Epoch 111/200\n",
      "7/7 [==============================] - 1s 73ms/step - loss: 0.0307 - accuracy: 0.9881 - val_loss: 0.9026 - val_accuracy: 0.8333\n",
      "Epoch 112/200\n",
      "7/7 [==============================] - 1s 72ms/step - loss: 0.0205 - accuracy: 0.9881 - val_loss: 0.8985 - val_accuracy: 0.8333\n",
      "Epoch 113/200\n",
      "7/7 [==============================] - 1s 74ms/step - loss: 0.0092 - accuracy: 1.0000 - val_loss: 0.8978 - val_accuracy: 0.8333\n",
      "Epoch 114/200\n",
      "7/7 [==============================] - 1s 73ms/step - loss: 0.0093 - accuracy: 1.0000 - val_loss: 0.9245 - val_accuracy: 0.8333\n",
      "Epoch 115/200\n",
      "7/7 [==============================] - 0s 68ms/step - loss: 0.0053 - accuracy: 1.0000 - val_loss: 0.9716 - val_accuracy: 0.8333\n",
      "Epoch 116/200\n",
      "7/7 [==============================] - 0s 69ms/step - loss: 0.0143 - accuracy: 1.0000 - val_loss: 1.0292 - val_accuracy: 0.8333\n",
      "Epoch 117/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 69ms/step - loss: 0.0042 - accuracy: 1.0000 - val_loss: 1.0684 - val_accuracy: 0.8333\n",
      "Epoch 118/200\n",
      "7/7 [==============================] - 0s 59ms/step - loss: 0.0166 - accuracy: 0.9881 - val_loss: 1.0619 - val_accuracy: 0.8333\n",
      "Epoch 119/200\n",
      "7/7 [==============================] - 0s 70ms/step - loss: 0.0102 - accuracy: 1.0000 - val_loss: 1.0707 - val_accuracy: 0.8333\n",
      "Epoch 120/200\n",
      "7/7 [==============================] - 0s 71ms/step - loss: 0.0042 - accuracy: 1.0000 - val_loss: 1.0736 - val_accuracy: 0.8333\n",
      "Epoch 121/200\n",
      "7/7 [==============================] - 0s 71ms/step - loss: 0.0096 - accuracy: 1.0000 - val_loss: 1.0989 - val_accuracy: 0.8333\n",
      "Epoch 122/200\n",
      "7/7 [==============================] - 0s 70ms/step - loss: 0.0109 - accuracy: 1.0000 - val_loss: 1.1329 - val_accuracy: 0.8333\n",
      "Epoch 123/200\n",
      "7/7 [==============================] - 0s 67ms/step - loss: 0.0337 - accuracy: 0.9762 - val_loss: 1.1504 - val_accuracy: 0.8333\n",
      "Epoch 124/200\n",
      "7/7 [==============================] - 0s 66ms/step - loss: 0.0124 - accuracy: 0.9881 - val_loss: 0.9650 - val_accuracy: 0.8889\n",
      "Epoch 125/200\n",
      "7/7 [==============================] - 1s 72ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.8994 - val_accuracy: 0.8333\n",
      "Epoch 126/200\n",
      "7/7 [==============================] - 0s 69ms/step - loss: 0.0143 - accuracy: 0.9881 - val_loss: 0.7594 - val_accuracy: 0.8333\n",
      "Epoch 127/200\n",
      "7/7 [==============================] - 0s 71ms/step - loss: 0.0099 - accuracy: 1.0000 - val_loss: 0.6601 - val_accuracy: 0.8333\n",
      "Epoch 128/200\n",
      "7/7 [==============================] - 0s 71ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.6783 - val_accuracy: 0.8333\n",
      "Epoch 129/200\n",
      "7/7 [==============================] - 0s 70ms/step - loss: 0.0102 - accuracy: 0.9881 - val_loss: 0.7192 - val_accuracy: 0.8333\n",
      "Epoch 130/200\n",
      "7/7 [==============================] - 1s 75ms/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 0.7583 - val_accuracy: 0.8333\n",
      "Epoch 131/200\n",
      "7/7 [==============================] - 0s 70ms/step - loss: 9.4591e-04 - accuracy: 1.0000 - val_loss: 0.8359 - val_accuracy: 0.8333\n",
      "Epoch 132/200\n",
      "7/7 [==============================] - 1s 73ms/step - loss: 7.9825e-04 - accuracy: 1.0000 - val_loss: 0.9116 - val_accuracy: 0.8333\n",
      "Epoch 133/200\n",
      "7/7 [==============================] - 1s 72ms/step - loss: 2.2509e-04 - accuracy: 1.0000 - val_loss: 0.9545 - val_accuracy: 0.8333\n",
      "Epoch 134/200\n",
      "7/7 [==============================] - 1s 74ms/step - loss: 0.0043 - accuracy: 1.0000 - val_loss: 1.0527 - val_accuracy: 0.8333\n",
      "Epoch 135/200\n",
      "7/7 [==============================] - 0s 68ms/step - loss: 0.0039 - accuracy: 1.0000 - val_loss: 1.2311 - val_accuracy: 0.8333\n",
      "Epoch 136/200\n",
      "7/7 [==============================] - 0s 69ms/step - loss: 7.8744e-04 - accuracy: 1.0000 - val_loss: 1.4391 - val_accuracy: 0.8333\n",
      "Epoch 137/200\n",
      "7/7 [==============================] - 1s 73ms/step - loss: 0.0128 - accuracy: 0.9881 - val_loss: 1.6057 - val_accuracy: 0.8333\n",
      "Epoch 138/200\n",
      "7/7 [==============================] - 1s 73ms/step - loss: 0.0087 - accuracy: 0.9881 - val_loss: 1.6839 - val_accuracy: 0.7778\n",
      "Epoch 139/200\n",
      "7/7 [==============================] - 1s 72ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 1.7228 - val_accuracy: 0.7778\n",
      "Epoch 140/200\n",
      "7/7 [==============================] - 1s 72ms/step - loss: 0.0033 - accuracy: 1.0000 - val_loss: 1.8038 - val_accuracy: 0.7778\n",
      "Epoch 141/200\n",
      "7/7 [==============================] - 0s 69ms/step - loss: 6.5944e-04 - accuracy: 1.0000 - val_loss: 1.8509 - val_accuracy: 0.7778\n",
      "Epoch 142/200\n",
      "7/7 [==============================] - 0s 69ms/step - loss: 0.0076 - accuracy: 1.0000 - val_loss: 1.8828 - val_accuracy: 0.7778\n",
      "Epoch 143/200\n",
      "7/7 [==============================] - 1s 75ms/step - loss: 0.0076 - accuracy: 1.0000 - val_loss: 1.7838 - val_accuracy: 0.7778\n",
      "Epoch 144/200\n",
      "7/7 [==============================] - 0s 72ms/step - loss: 3.0960e-04 - accuracy: 1.0000 - val_loss: 1.7406 - val_accuracy: 0.7778\n",
      "Epoch 145/200\n",
      "7/7 [==============================] - 0s 70ms/step - loss: 0.0058 - accuracy: 1.0000 - val_loss: 1.7482 - val_accuracy: 0.7778\n",
      "Epoch 146/200\n",
      "7/7 [==============================] - 1s 73ms/step - loss: 0.0051 - accuracy: 1.0000 - val_loss: 1.7604 - val_accuracy: 0.7778\n",
      "Epoch 147/200\n",
      "7/7 [==============================] - 1s 73ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 1.7969 - val_accuracy: 0.7778\n",
      "Epoch 148/200\n",
      "7/7 [==============================] - 1s 76ms/step - loss: 4.2647e-04 - accuracy: 1.0000 - val_loss: 1.8229 - val_accuracy: 0.7778\n",
      "Epoch 149/200\n",
      "7/7 [==============================] - 0s 70ms/step - loss: 7.4847e-04 - accuracy: 1.0000 - val_loss: 1.8568 - val_accuracy: 0.7778\n",
      "Epoch 150/200\n",
      "7/7 [==============================] - 0s 71ms/step - loss: 5.9824e-04 - accuracy: 1.0000 - val_loss: 1.8833 - val_accuracy: 0.7778\n",
      "Epoch 151/200\n",
      "7/7 [==============================] - 0s 71ms/step - loss: 8.6009e-04 - accuracy: 1.0000 - val_loss: 1.8975 - val_accuracy: 0.7778\n",
      "Epoch 152/200\n",
      "7/7 [==============================] - 1s 72ms/step - loss: 4.4955e-04 - accuracy: 1.0000 - val_loss: 1.9082 - val_accuracy: 0.7778\n",
      "Epoch 153/200\n",
      "7/7 [==============================] - 1s 73ms/step - loss: 0.0084 - accuracy: 1.0000 - val_loss: 1.9228 - val_accuracy: 0.7778\n",
      "Epoch 154/200\n",
      "7/7 [==============================] - 0s 70ms/step - loss: 0.0078 - accuracy: 1.0000 - val_loss: 1.9525 - val_accuracy: 0.7778\n",
      "Epoch 155/200\n",
      "7/7 [==============================] - 1s 73ms/step - loss: 0.0109 - accuracy: 1.0000 - val_loss: 2.0310 - val_accuracy: 0.7778\n",
      "Epoch 156/200\n",
      "7/7 [==============================] - 1s 74ms/step - loss: 5.3191e-04 - accuracy: 1.0000 - val_loss: 2.1200 - val_accuracy: 0.7778\n",
      "Epoch 157/200\n",
      "7/7 [==============================] - 1s 74ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 2.2052 - val_accuracy: 0.7778\n",
      "Epoch 158/200\n",
      "7/7 [==============================] - 1s 73ms/step - loss: 2.6765e-04 - accuracy: 1.0000 - val_loss: 2.2603 - val_accuracy: 0.7778\n",
      "Epoch 159/200\n",
      "7/7 [==============================] - 0s 70ms/step - loss: 3.9634e-05 - accuracy: 1.0000 - val_loss: 2.2890 - val_accuracy: 0.7778\n",
      "Epoch 160/200\n",
      "7/7 [==============================] - 0s 71ms/step - loss: 4.5919e-04 - accuracy: 1.0000 - val_loss: 2.3014 - val_accuracy: 0.7778\n",
      "Epoch 161/200\n",
      "7/7 [==============================] - 1s 74ms/step - loss: 0.0040 - accuracy: 1.0000 - val_loss: 2.2003 - val_accuracy: 0.7778\n",
      "Epoch 162/200\n",
      "7/7 [==============================] - 0s 62ms/step - loss: 1.2199e-04 - accuracy: 1.0000 - val_loss: 2.1221 - val_accuracy: 0.7778\n",
      "Epoch 163/200\n",
      "7/7 [==============================] - 0s 60ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 2.0542 - val_accuracy: 0.8333\n",
      "Epoch 164/200\n",
      "7/7 [==============================] - 0s 72ms/step - loss: 0.0040 - accuracy: 1.0000 - val_loss: 2.0209 - val_accuracy: 0.8333\n",
      "Epoch 165/200\n",
      "7/7 [==============================] - 0s 70ms/step - loss: 9.0637e-04 - accuracy: 1.0000 - val_loss: 2.0479 - val_accuracy: 0.8333\n",
      "Epoch 166/200\n",
      "7/7 [==============================] - 0s 71ms/step - loss: 5.1037e-05 - accuracy: 1.0000 - val_loss: 2.0826 - val_accuracy: 0.7778\n",
      "Epoch 167/200\n",
      "7/7 [==============================] - 0s 73ms/step - loss: 2.0079e-04 - accuracy: 1.0000 - val_loss: 2.0974 - val_accuracy: 0.7778\n",
      "Epoch 168/200\n",
      "7/7 [==============================] - 0s 70ms/step - loss: 0.0082 - accuracy: 1.0000 - val_loss: 2.1044 - val_accuracy: 0.7778\n",
      "Epoch 169/200\n",
      "7/7 [==============================] - 1s 72ms/step - loss: 8.0809e-05 - accuracy: 1.0000 - val_loss: 2.1099 - val_accuracy: 0.7778\n",
      "Epoch 170/200\n",
      "7/7 [==============================] - 1s 74ms/step - loss: 6.4755e-04 - accuracy: 1.0000 - val_loss: 2.1183 - val_accuracy: 0.7778\n",
      "Epoch 171/200\n",
      "7/7 [==============================] - 0s 66ms/step - loss: 0.0039 - accuracy: 1.0000 - val_loss: 2.1406 - val_accuracy: 0.7778\n",
      "Epoch 172/200\n",
      "7/7 [==============================] - 0s 71ms/step - loss: 0.0104 - accuracy: 0.9881 - val_loss: 2.3184 - val_accuracy: 0.7778\n",
      "Epoch 173/200\n",
      "7/7 [==============================] - 0s 70ms/step - loss: 4.2594e-05 - accuracy: 1.0000 - val_loss: 2.3967 - val_accuracy: 0.7778\n",
      "Epoch 174/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 70ms/step - loss: 4.2690e-05 - accuracy: 1.0000 - val_loss: 2.4208 - val_accuracy: 0.7778\n",
      "Epoch 175/200\n",
      "7/7 [==============================] - 0s 70ms/step - loss: 3.9286e-04 - accuracy: 1.0000 - val_loss: 2.4241 - val_accuracy: 0.7778\n",
      "Epoch 176/200\n",
      "7/7 [==============================] - 1s 73ms/step - loss: 9.4200e-04 - accuracy: 1.0000 - val_loss: 2.4548 - val_accuracy: 0.7778\n",
      "Epoch 177/200\n",
      "7/7 [==============================] - 0s 70ms/step - loss: 4.3979e-05 - accuracy: 1.0000 - val_loss: 2.4794 - val_accuracy: 0.7778\n",
      "Epoch 178/200\n",
      "7/7 [==============================] - 1s 74ms/step - loss: 1.8158e-04 - accuracy: 1.0000 - val_loss: 2.4778 - val_accuracy: 0.7778\n",
      "Epoch 179/200\n",
      "7/7 [==============================] - 0s 68ms/step - loss: 5.2081e-05 - accuracy: 1.0000 - val_loss: 2.4636 - val_accuracy: 0.7778\n",
      "Epoch 180/200\n",
      "7/7 [==============================] - 0s 67ms/step - loss: 0.0252 - accuracy: 0.9881 - val_loss: 2.1991 - val_accuracy: 0.8333\n",
      "Epoch 181/200\n",
      "7/7 [==============================] - 1s 72ms/step - loss: 0.0056 - accuracy: 1.0000 - val_loss: 1.9367 - val_accuracy: 0.8333\n",
      "Epoch 182/200\n",
      "7/7 [==============================] - 0s 71ms/step - loss: 2.4381e-04 - accuracy: 1.0000 - val_loss: 1.7769 - val_accuracy: 0.7778\n",
      "Epoch 183/200\n",
      "7/7 [==============================] - 0s 66ms/step - loss: 0.0028 - accuracy: 1.0000 - val_loss: 1.7063 - val_accuracy: 0.7778\n",
      "Epoch 184/200\n",
      "7/7 [==============================] - 1s 74ms/step - loss: 0.0083 - accuracy: 1.0000 - val_loss: 1.6858 - val_accuracy: 0.7778\n",
      "Epoch 185/200\n",
      "7/7 [==============================] - 0s 70ms/step - loss: 8.6811e-04 - accuracy: 1.0000 - val_loss: 1.6868 - val_accuracy: 0.7778\n",
      "Epoch 186/200\n",
      "7/7 [==============================] - 0s 71ms/step - loss: 5.5321e-05 - accuracy: 1.0000 - val_loss: 1.7007 - val_accuracy: 0.7778\n",
      "Epoch 187/200\n",
      "7/7 [==============================] - 1s 72ms/step - loss: 0.0084 - accuracy: 0.9881 - val_loss: 1.7242 - val_accuracy: 0.7778\n",
      "Epoch 188/200\n",
      "7/7 [==============================] - 0s 71ms/step - loss: 0.0036 - accuracy: 1.0000 - val_loss: 1.7732 - val_accuracy: 0.7778\n",
      "Epoch 189/200\n",
      "7/7 [==============================] - 1s 72ms/step - loss: 3.9779e-05 - accuracy: 1.0000 - val_loss: 1.8127 - val_accuracy: 0.7778\n",
      "Epoch 190/200\n",
      "7/7 [==============================] - 0s 66ms/step - loss: 5.0107e-04 - accuracy: 1.0000 - val_loss: 1.8331 - val_accuracy: 0.7778\n",
      "Epoch 191/200\n",
      "7/7 [==============================] - 0s 71ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 1.8598 - val_accuracy: 0.7778\n",
      "Epoch 192/200\n",
      "7/7 [==============================] - 0s 71ms/step - loss: 2.0808e-05 - accuracy: 1.0000 - val_loss: 1.8857 - val_accuracy: 0.7778\n",
      "Epoch 193/200\n",
      "7/7 [==============================] - 0s 71ms/step - loss: 3.5287e-04 - accuracy: 1.0000 - val_loss: 1.8963 - val_accuracy: 0.7778\n",
      "Epoch 194/200\n",
      "7/7 [==============================] - 1s 72ms/step - loss: 1.7102e-04 - accuracy: 1.0000 - val_loss: 1.8945 - val_accuracy: 0.7778\n",
      "Epoch 195/200\n",
      "7/7 [==============================] - 0s 72ms/step - loss: 6.9247e-04 - accuracy: 1.0000 - val_loss: 1.9007 - val_accuracy: 0.7778\n",
      "Epoch 196/200\n",
      "7/7 [==============================] - 1s 73ms/step - loss: 5.7720e-05 - accuracy: 1.0000 - val_loss: 1.9362 - val_accuracy: 0.7778\n",
      "Epoch 197/200\n",
      "7/7 [==============================] - 1s 72ms/step - loss: 1.2406e-04 - accuracy: 1.0000 - val_loss: 1.9584 - val_accuracy: 0.7778\n",
      "Epoch 198/200\n",
      "7/7 [==============================] - 0s 60ms/step - loss: 1.9767e-05 - accuracy: 1.0000 - val_loss: 1.9742 - val_accuracy: 0.7778\n",
      "Epoch 199/200\n",
      "7/7 [==============================] - 0s 71ms/step - loss: 8.5777e-05 - accuracy: 1.0000 - val_loss: 1.9885 - val_accuracy: 0.7778\n",
      "Epoch 200/200\n",
      "7/7 [==============================] - 0s 64ms/step - loss: 1.0641e-05 - accuracy: 1.0000 - val_loss: 2.0007 - val_accuracy: 0.7778\n"
     ]
    }
   ],
   "source": [
    "# Destroys the current TF graph and creates a new one. Useful to avoid clutter from old models / layers\n",
    "K.clear_session()\n",
    "# Set learning phase from our model to training\n",
    "K.set_learning_phase(1)\n",
    "\n",
    "# Create our Modified LeNet-5 model\n",
    "model = Sequential()\n",
    "\n",
    "# Add blocks of convolutional layers followed by batch normalization and pooling\n",
    "model.add(Conv2D(filters, kernel_size, activation='relu', input_shape=data_shape))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D())\n",
    "\n",
    "model.add(Conv2D(filters * 2, kernel_size, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D())\n",
    "\n",
    "model.add(Conv2D(filters * 4, kernel_size, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D())\n",
    "\n",
    "# Reshape the tensor to have the shape that is equal to the number of elements contained \n",
    "# in tensor non including the batch dimension\n",
    "model.add(Flatten())\n",
    "\n",
    "# Add the fully connected layer followed by a dropout of 50%\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# We finally add the softmax classifier to our model\n",
    "model.add(Dense(n_classes, activation='sigmoid', name='preds'))\n",
    "\n",
    "# Define the learning rate to our optimizer and which optimizer we will use\n",
    "from keras import optimizers\n",
    "learning_rate = 1e-5\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "# Here we configure the loss and metrics of our model\n",
    "model.compile(optimizer=\"adam\",\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "history = model.fit(X_train, y_train, validation_data=(X_val, y_val),\n",
    "              epochs = 200,\n",
    "              batch_size= 12,\n",
    "              verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8c6be97a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 33ms/step - loss: 0.6318 - accuracy: 0.9444\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6318085193634033, 0.9444444179534912]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "94b1f860",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 123ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1.00000000e+00, 7.12741033e-10],\n",
       "       [1.00000000e+00, 7.38549291e-11],\n",
       "       [1.00000000e+00, 2.88932096e-08],\n",
       "       [1.00000000e+00, 6.62218707e-13],\n",
       "       [1.00000000e+00, 1.00119095e-07],\n",
       "       [1.00000000e+00, 3.66651015e-11],\n",
       "       [9.99999702e-01, 1.82172664e-06],\n",
       "       [1.00000000e+00, 1.25181948e-11],\n",
       "       [1.00000000e+00, 2.30819612e-14],\n",
       "       [3.21415996e-08, 9.99999881e-01],\n",
       "       [3.45746885e-05, 9.99958038e-01],\n",
       "       [1.19454392e-12, 1.00000000e+00],\n",
       "       [1.39374231e-06, 9.99995291e-01],\n",
       "       [1.63204297e-10, 1.00000000e+00],\n",
       "       [2.60283002e-07, 9.99999106e-01],\n",
       "       [2.59300936e-09, 1.00000000e+00],\n",
       "       [9.99994516e-01, 4.34115100e-05],\n",
       "       [3.36368412e-01, 8.39219451e-01]], dtype=float32)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Analyze prediction values\n",
    "\n",
    "\n",
    "\n",
    "predicted = model.predict(X_test)\n",
    "predicted[:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "105ff755",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 43ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      1.00      0.95         9\n",
      "           1       1.00      0.89      0.94         9\n",
      "\n",
      "    accuracy                           0.94        18\n",
      "   macro avg       0.95      0.94      0.94        18\n",
      "weighted avg       0.95      0.94      0.94        18\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import confusion matrix and classification report from scikit-learn\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "Y_prediction = model.predict(X_test)\n",
    "# Convert predictions classes to one hot vectors \n",
    "Y_pred_classes = np.argmax(predicted,axis = 1) \n",
    "# compute the confusion matrix\n",
    "confusion_mtx = confusion_matrix(target[indexes_test], Y_pred_classes) \n",
    "print(classification_report(target[indexes_test], Y_pred_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0c313639",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x800 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(10,8))\n",
    "sns.heatmap(confusion_mtx, annot=True, fmt=\"d\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3d0c54",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
